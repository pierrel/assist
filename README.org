#+Title: Assist
Assist is an extensible, local-focused, llm-based assistant. The principles guiding this project include:
- Privacy - The user has full control and no information leaves their computer.
- Extensibility - All agentic behavior is easy to modify or add to.
- Generic - Assist can help with anything.

It currently provides an OpenAI-compatible API with a ReAct agent and some simple tools, including a safe Python execution tool for read-only computations with limited builtins (e.g., =abs=, =sum=, =pow=) and modules (=math=, =statistics=, =random=, =numpy= if available). To capture a value, assign it to a variable named =result= or print it. A small demonstration lives in =playground/play_safe_python.py= using =ChatOpenAI= with the safe Python tool to compute compound savings. It's currently very tailored to Emacs, but hopefully can be extended in the future to handle other editors.

Streaming responses now send only the assistant's latest message rather than replaying prior conversation history.

It is not /just/ a coding assistant, but should be able to help with any project or goal.
When running the server, built-in tools refuse to read from or write to the Assist codebase itself for safety. The planner and executor prompts explicitly forbid using any tool to access that directory.
This warning is only shown when the server knows its project root.
* Concepts
** Project
A project is a git repository, contains the main context for interactions as well as external references. All of the interaction with Assist is handled within the context of the project.

A user is /in/ the project when they are actively editing the files. Right now there is some support for emacs through inspecting the currently active buffers to gain context.
** Context
The information needed to perform a task. This is inclusive of the main project files, other local files, and internet resources.
** Agent
The agent acts within the context of the project and can perform internet searches. It takes a requests from the user to perform tasks.
** Task
A unit of work for the agent to perform on behalf of the user.
* Getting started
The set-up is currently fairly manual. Clone and follow the below.
** Dependencies
Install dependencies

#+begin_src shell
pip install -r requirements.txt
#+end_src

Typing stubs for ``requests`` and ``python-dateutil`` are included to enable
stricter type checking.

Enable the virtual environment

#+begin_src shell
source .venv/bin/activate
#+end_src

In emacs: =M-x pyvenv-activate=
** Running
Run the server

#+begin shell
python src/assist/server.python
#+end_shell
** Developing
Install the package in editable mode so Emacs/Elpy can discover the modules:

#+begin_src shell
pip install -e .
#+end_src

Run the tests with coverage:

#+begin_src shell
pytest
#+end_src

Generate a static type report:

#+begin_src shell
mypy src
#+end_src

Both coverage and type checking run in CI for pull requests and pushes to main.
Reports are uploaded as artifacts and do not block the build.

Run the integration tests with:

#+begin_src shell
pytest tests/integration
#+end_src

Additional validation tests for the planner node cover ten example scenarios.
Each scenario asserts that plans query external references (e.g., Tavily search or local indexes),
so they require valid `OPENAI_API_KEY` and `TAVILY_API_KEY` environment variables:

#+begin_src shell
pytest tests/integration/validation/test_planner_node.py
#+end_src

** Debugging
To see every prompt and response during an agent run, use
`ReadableConsoleCallbackHandler`. Each LLM call prints the node, model, prompt,
and response in a readable format.

** Validations
There's a set of tests in =tests/integration/validations= that stress a live llm within the agent and different nodes in different conditions.

Run the whole validation suite repeatedly to check stability:

#+begin_src shell
python scripts/run_validation_tests.py        # runs 10 times by default
python scripts/run_validation_tests.py 5      # run it five times
#+end_src
** Examples
Small scripts under =playground/= demonstrate different pieces of the system.
To see how a tool error is surfaced, run:

#+begin_src shell
python playground/tool_exception.py
#+end_src

The script defines a tool that always fails, wires a live model to it, and prints the
resulting messages, including the tool error. Provide a valid `OPENAI_API_KEY` in the
environment.
* User flows
These are the main user flows for working with Assist
** Re-write
I want to highlight a region and ask that it be re-written in a certain way.
** Explain/describe
When I first open a project, I want to have a high-level overview of it. This should be fairly straightforward to ask while working on the project. Probably should generate automatically when there is no explanation or when the explanation was created long before the current version (check git?).
** Suggest
Make recommendations on what to do next or what to update as you're editing. Find bugs and make the recommendations somewhere.
** Large changes
I want to be able to ask for meaningfully large changes like refactors or implementation of whole features.
* Roadmap
** Guidance
/Under active development/

Tasks primarily involve gathering information from relevant sources and providing guidance to the user. They can be thought of as side-effect free.
*** Objectives
**** TODO Improve summarization step
Better prompt, currently it's the simplest version.
**** TODO Improve tool understanding and usage
Better descriptions.
**** TODO Simplify node/prompt/graph interaction and composition
Less verbose, more testable.
**** TODO Prompt and flow optimization
Need a framework for automatically judging results and
**** TODO System message to llm
Currently the supplied system message is not transferred to the agent. Either do it or be explicit about it being thrown away.
**** TODO Automatic llm selection
Based on the task. Currenlty 1 llm runs for everything, which is either overkill or insufficient for some tasks.
**** TODO Multi-turn interactions
It's currently optimized to answer 1 thing without a history of other asks.
**** TODO Cross-project user preferences
**** TODO External tools
On-computer "manuals" that can be shared across projects.
**** TODO Handle "re-write" use-case
Offer an in-emacs diff of a proposed change based on the current region, the project, and the request.
**** TODO User preferences and facts
An agent that can store and retrieve information that will better help.
** Action
/Future feature/

Actually perform some work which has side effects. These could be:
- Editing files (within the project)
- Making API requests which have side effects
- Opening a browser to do work
*** Within the project
*** Outside the project
** Proactivity
/Future feature/

Perform unsolicited work for the user. For example, analyze the current project and decide what the user would do next to get closer to their goal(s).

The results could be in the form of:
- Proposed change to the project files (like a PR)
- Recommended purchases (just hit "OK" to actually do it)
